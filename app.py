
import streamlit as st
import trafilatura
from bs4 import BeautifulSoup
import requests
import os.path
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
import json

# --- Config ---
st.set_page_config(page_title="Web Archive Helper", page_icon="ğŸ“", layout="wide")

# --- Custom CSS for Font Size ---
st.markdown("""
    <style>
    html, body, [class*="css"]  {
        font-size: 20px !important;
    }
    .stTextInput > div > div > input {
        font-size: 20px !important;
    }
    .stTextArea > div > div > textarea {
        font-size: 18px !important;
    }
    .stButton > button {
        font-size: 20px !important;
        height: 3em !important;
    }
    .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {
        font-size: 1.25em !important; 
    }
    .stMarkdown p {
        font-size: 20px !important;
    }
    </style>
    """, unsafe_allow_html=True)


SCOPES = ['https://www.googleapis.com/auth/documents', 'https://www.googleapis.com/auth/drive']
DOC_NAME_TARGET = 'í€€íŠ¸ ë¸”ë¡œê·¸ í•™ìŠµ (ì „ëµ) íŒŒì¼'

# --- Functions ---

def clean_xml_to_markdown(xml_string):
    """
    Trafilatura returns a simplified XML. We convert this to Markdown.
    Focus on p, head, list, table, graphic.
    """
    if not xml_string:
        return ""
    
    soup = BeautifulSoup(xml_string, 'xml')
    md_output = ""
    
    # Trafilatura XML structure is usually <doc> or <main>
    # We iterate over children
    root = soup.find('doc') or soup.find('main')
    if not root:
        return ""
        
    for element in root.descendants:
        # We only process tags, but descendants includes all. 
        # Better strategy: iterate recursively or just linear pass if flat.
        # Trafilatura output is relatively flat but has nesting (lists, tables).
        # Let's use a simpler mapping strategy: direct extraction using markdownify logic wouldn't work on XML tags directly.
        pass
    
    # Alternative: Trafilatura has its own 'output_format="json"' which gives structure?
    # Or better: Trafilatura's XML is close to HTML. Let's converting XML tags to HTML tags then markdownify.
    
    # Rename tags for markdownify
    # <graphic src="..."> -> <img src="...">
    # <head> -> <h3> (or similar)
    # <p> is <p>
    # <list> is <ul>, <item> is <li>
    # <table> is <table>
    
    xml_str_mod = str(root)
    xml_str_mod = xml_str_mod.replace('<graphic', '<img').replace('</graphic>', '</img>')
    xml_str_mod = xml_str_mod.replace('<head', '<h2').replace('</head>', '</h2>')
    xml_str_mod = xml_str_mod.replace('<list', '<ul').replace('</list>', '</ul>')
    xml_str_mod = xml_str_mod.replace('<item', '<li').replace('</item>', '</li>')
    # Row/Cell mapping if needed, but trafilatura usually preserves table structure or simplifies it.
    
    # Now pass to markdownify (we need to import it)
    from markdownify import markdownify
    # Markdownify expects HTML-like soup or string
    return markdownify(xml_str_mod, heading_style="ATX")

def extract_content(url):
    try:
        # Use cloudscraper to bypass Cloudflare/WAF checks
        import cloudscraper
        scraper = cloudscraper.create_scraper() 
        
        # 1. Fetch with Cloudscraper
        response = scraper.get(url, timeout=15)
        response.raise_for_status()
        html_content = response.text
        
        # 2. Extract with Trafilatura
        # include_images=True, include_tables=True, output_format='xml'
        result = trafilatura.extract(html_content, include_images=True, include_tables=True, output_format='xml')
        
        if not result:
             # Fallback: Try trafilatura's native fetch if requests fail to provide good content (unlikely but safe)
             downloaded = trafilatura.fetch_url(url)
             if downloaded:
                 result = trafilatura.extract(downloaded, include_images=True, include_tables=True, output_format='xml')

        if not result:
            # Let's verify what we got - maybe return a snippet of the HTML to debug if it fails again
            soup = BeautifulSoup(html_content, 'html.parser')
            text_preview = soup.get_text()[:500].strip()
            return None, f"Error: Could not extract content. Site returned: {text_preview}..."
            
        # Get Metadata (Title)
        # We can extract title from html_content directly or use trafilatura
        # trafilatura.extract_metadata expects the HTML string
        meta = trafilatura.extract_metadata(html_content)
        title = meta.title if meta else "Untitled"
        
        # Convert XML to MD
        md_content = clean_xml_to_markdown(result)
        
        final_md = f"# {title}\n\nURL: {url}\n\n{md_content}"
        return final_md, None
        
    except Exception as e:
        return None, f"Error fetching URL: {str(e)}"

def get_google_creds():
    creds = None
    
    # 1. Try Streamlit Secrets (for Cloud)
    if "google_token" in st.secrets:
        try:
            # We assume the user pasted the content of token.json into a secret named "google_token"
            token_info = json.loads(st.secrets["google_token"])
            creds = Credentials.from_authorized_user_info(token_info, SCOPES)
            return creds
        except Exception as e:
            st.error(f"Secrets Error: {str(e)}")
            return None

    # 2. Try Local File (for PC)
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', SCOPES)
    
    # If there are no (valid) credentials available, let the user log in (ONLY LOCAL).
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            try:
                creds.refresh(Request())
            except:
                creds = None
        
        if not creds:
            if os.path.exists('credentials.json'):
                flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)
                creds = flow.run_local_server(port=0)
                # Save the credentials for the next run
                with open('token.json', 'w') as token:
                    token.write(creds.to_json())
            else:
                return None
    return creds

def append_to_doc(md_content):
    creds = get_google_creds()
    if not creds:
        return False, "ì¸ì¦ ì‹¤íŒ¨: credentials.json íŒŒì¼ì„ í”„ë¡œì íŠ¸ í´ë”ì— ë„£ì–´ì£¼ì„¸ìš”."

    try:
        service = build('docs', 'v1', credentials=creds)
        drive_service = build('drive', 'v3', credentials=creds)

        # 1. Find the file
        results = drive_service.files().list(
            q=f"name = '{DOC_NAME_TARGET}' and mimeType = 'application/vnd.google-apps.document'",
            fields="nextPageToken, files(id, name)").execute()
        items = results.get('files', [])

        if not items:
            return False, f"êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ '{DOC_NAME_TARGET}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        doc_id = items[0]['id']

        # 2. Append content
        # Docs API insertText handles plain text suitable. Markdown formatting won't be applied automatically.
        # But we append the text content.
        # To make it slightly better, we can insert a page break before.
        
        doc = service.documents().get(documentId=doc_id).execute()
        content = doc.get('body').get('content')
        end_index = content[-1]['endIndex'] - 1
        
        text_to_insert = "\n\n" + "-"*20 + "\n\n" + md_content
        
        requests = [
            {
                'insertText': {
                    'location': {
                        'index': end_index
                    },
                    'text': text_to_insert
                }
            },
            {
                'updateTextStyle': {
                    'range': {
                        'startIndex': end_index,
                        'endIndex': end_index + len(text_to_insert)
                    },
                    'textStyle': {
                        'fontSize': {
                            'magnitude': 12, # 10pt is default, boosting to 12pt
                            'unit': 'PT'
                        }
                    },
                    'fields': 'fontSize'
                }
            }
        ]

        service.documents().batchUpdate(documentId=doc_id, body={'requests': requests}).execute()
        return True, "ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. (ê¸€ì í¬ê¸° 12pt ì ìš©)"
        
    except Exception as e:
        return False, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# --- UI ---

st.title("ğŸ¤– Web Content Archiver to Google Drive")
st.markdown("URLì„ ì…ë ¥í•˜ë©´ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ Markdownìœ¼ë¡œ ë³€í™˜í•˜ê³ , êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë¬¸ì„œì— ì´ì–´ë¶™ì…ë‹ˆë‹¤.")

url = st.text_input("URLì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="https://example.com/...")

if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None

if st.button("ë¶„ì„ ë° ë³€í™˜ ì‹œì‘"):
    if url:
        with st.spinner('ì›¹í˜ì´ì§€ ë¶„ì„ & ì •ì œ ì¤‘...'):
            md_result, error = extract_content(url)
            if error:
                st.error(error)
            else:
                st.session_state.processed_data = md_result
                st.success("ë³€í™˜ ì™„ë£Œ!")
    else:
        st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

if st.session_state.processed_data:
    st.subheader("ğŸ“ ì¶”ì¶œëœ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
    st.text_area("Markdown Content", st.session_state.processed_data, height=400)
    
    col1, col2 = st.columns(2)
    
    # Download Button
    file_name = "extracted_content.md"
    col1.download_button(
        label="ğŸ“¥ Markdown íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
        data=st.session_state.processed_data,
        file_name=file_name,
        mime="text/markdown"
    )
    
    # Google Drive Append Button
    if col2.button("â˜ï¸ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì¶”ê°€"):
        with st.spinner('êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì—°ê²° ì¤‘...'):
            success, msg = append_to_doc(st.session_state.processed_data)
            if success:
                st.success(f"{msg}")
                st.balloons()
            else:
                st.error(f"{msg}")
    
    if st.button("ğŸ”„ ì´ˆê¸°í™” (ë‹¤ìŒ URL)"):
        st.session_state.processed_data = None
        st.rerun()

